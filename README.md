[Bayesian compression for deep learning (Louizos , Welling; 2017).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Bayesian compression for deep learning (Louizos , Welling; 
2017).md)

[Deep Compression: Pruning, trained quantization, huffman coding (Han et al 2016).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Deep Compression: Pruning, trained quantization, 
huffman coding (Han et al 2016).md)
[Drawing Early-bird tickets (You et al.).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Drawing Early-bird tickets (You et al.).md)

[Evaluating Lottery Tickets Under Distributional Shifts (Desai et al 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Evaluating Lottery Tickets Under Distributional Shifts (
Desai et al 2019).md)

[Learning Sparse Neural Networks Through L0 Regularization (Welling, Kingma, Louizos; 2018).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Learning Sparse Neural Networks Through L0 
Regularization (Welling, Kingma, Louizos; 2018).md)

[One ticket to win them all (Ari et al, Facebook Research, 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/One ticket to win them all (Ari et al, Facebook Research, 2019).md)

[Optimal Brain Damage (LeCun et al. ,1990).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Optimal Brain Damage (LeCun et al. ,1990).md)

[ORIGINAL: Lottery Ticket Hypothesis.md](https://github.com/StijnVerdenius/LTH_paper_summaries/ORIGINAL: Lottery Ticket Hypothesis.md)

[Rethinking the value of network pruning (Lui et al (Berkeley), 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Rethinking the value of network pruning (Lui et al (Berkeley), 
2019).md)

[Second order derivatives for network pruning: Optimal Brain Surgeon (Stork & Hassibi (stanford), 1993).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Second order derivatives for 
network pruning: Optimal Brain Surgeon (Stork & Hassibi (stanford), 1993).md)

[SNIP (Lee et al, 2018).md](https://github.com/StijnVerdenius/LTH_paper_summaries/SNIP (Lee et al, 2018).md)

[Sparse Networks from scratch (Dettmers et al, 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Sparse Networks from scratch (Dettmers et al, 2019).md)

[Stabilizing the Lottery Ticket Hypothesis (Frankle et al. 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Stabilizing the Lottery Ticket Hypothesis (Frankle et al. 2019).md)

[Targeted Dropout (Gomez et al., 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Targeted Dropout (Gomez et al., 2019).md)

[The State of Sparsity in Deep NNs (Gale et al 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/The State of Sparsity in Deep NNs (Gale et al 2019).md)

[To prune or not to prune: efficacy of pruning for model compression (Zhu, Gupta, 2017).md](https://github.com/StijnVerdenius/LTH_paper_summaries/To prune or not to prune: efficacy of pruning 
for model compression (Zhu, Gupta, 2017).md)

[Variational Dropout Sparsifies Deep Neural Network (Molchanov et al 2017).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Variational Dropout Sparsifies Deep Neural Network (
Molchanov et al 2017).md)

[Variational Dropout (Welling & Kingma, 2015).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Variational Dropout (Welling & Kingma, 2015).md)

[Zeros, signs and the supermask (Zhou et al (uber), 2019).md](https://github.com/StijnVerdenius/LTH_paper_summaries/Zeros, signs and the supermask (Zhou et al (uber), 2019).md)
