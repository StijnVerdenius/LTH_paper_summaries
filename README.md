
[Bayesian compression for deep learning (Louizos , Welling; 2017).md).](./Bayesian compression for deep learning (Louizos , Welling; 2017).md)
[Deep Compression: Pruning, trained quantization, huffman coding (Han et al 2016).md](./Deep Compression: Pruning, trained quantization, huffman coding (Han et al 2016).md)
[Drawing Early-bird tickets (You et al.).md](./Drawing Early-bird tickets (You et al.).md)
[Evaluating Lottery Tickets Under Distributional Shifts (Desai et al 2019).md](./Evaluating Lottery Tickets Under Distributional Shifts (Desai et al 2019).md)
[Learning Sparse Neural Networks Through L0 Regularization (Welling, Kingma, Louizos; 2018).md](./Learning Sparse Neural Networks Through L0 Regularization (Welling, Kingma, Louizos; 2018).md)
[One ticket to win them all (Ari et al, Facebook Research, 2019).md](./One ticket to win them all (Ari et al, Facebook Research, 2019).md)
[Optimal Brain Damage (LeCun et al. ,1990).md](./Optimal Brain Damage (LeCun et al. ,1990).md)
[ORIGINAL: Lottery Ticket Hypothesis.md](./ORIGINAL: Lottery Ticket Hypothesis.md)
[Rethinking the value of network pruning (Lui et al (Berkeley), 2019).md](./Rethinking the value of network pruning (Lui et al (Berkeley), 2019).md)
[Second order derivatives for network pruning: Optimal Brain Surgeon (Stork & Hassibi (stanford), 1993).md](./Second order derivatives for network pruning: Optimal Brain Surgeon (Stork & Hassibi (stanford), 1993).md)
[SNIP (Lee et al, 2018).md](./SNIP (Lee et al, 2018).md)
[Sparse Networks from scratch (Dettmers et al, 2019).md](./Sparse Networks from scratch (Dettmers et al, 2019).md)
[Stabilizing the Lottery Ticket Hypothesis (Frankle et al. 2019).md](./Stabilizing the Lottery Ticket Hypothesis (Frankle et al. 2019).md)
[Targeted Dropout (Gomez et al., 2019).md](./Targeted Dropout (Gomez et al., 2019).md)
[The State of Sparsity in Deep NNs (Gale et al 2019).md](./The State of Sparsity in Deep NNs (Gale et al 2019).md)
[To prune or not to prune: efficacy of pruning for model compression (Zhu, Gupta, 2017).md](./To prune or not to prune: efficacy of pruning for model compression (Zhu, Gupta, 2017).md)
[Variational Dropout Sparsifies Deep Neural Network (Molchanov et al 2017).md](./Variational Dropout Sparsifies Deep Neural Network (Molchanov et al 2017).md)
[Variational Dropout (Welling & Kingma, 2015).md](./Variational Dropout (Welling & Kingma, 2015).md)
[Zeros, signs and the supermask (Zhou et al (uber), 2019).md](./Zeros, signs and the supermask (Zhou et al (uber), 2019).md)
